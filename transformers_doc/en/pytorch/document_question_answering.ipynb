{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6VYB_wzD5b7",
        "outputId": "1eba1247-0432-4f03-ba52-0d3bedd2616b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets evaluate accelerate\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xQx_Y6tD5cB"
      },
      "source": [
        "# Document Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYMX_fncD5cE"
      },
      "source": [
        "Document Question Answering, also referred to as Document Visual Question Answering, is a task that involves providing\n",
        "answers to questions posed about document images. The input to models supporting this task is typically a combination of an image and\n",
        "a question, and the output is an answer expressed in natural language. These models utilize multiple modalities, including\n",
        "text, the positions of words (bounding boxes), and the image itself.\n",
        "\n",
        "This guide illustrates how to:\n",
        "\n",
        "- Fine-tune [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv2) on the [DocVQA dataset](https://huggingface.co/datasets/nielsr/docvqa_1200_examples_donut).\n",
        "- Use your fine-tuned model for inference.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "To see all architectures and checkpoints compatible with this task, we recommend checking the [task-page](https://huggingface.co/tasks/image-to-text)\n",
        "\n",
        "</Tip>\n",
        "\n",
        "LayoutLMv2 solves the document question-answering task by adding a question-answering head on top of the final hidden\n",
        "states of the tokens, to predict the positions of the start and end tokens of the\n",
        "answer. In other words, the problem is treated as extractive question answering: given the context, extract which piece\n",
        "of information answers the question. The context comes from the output of an OCR engine, here it is Google's Tesseract.\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed. LayoutLMv2 depends on detectron2, torchvision and tesseract.\n",
        "\n",
        "```bash\n",
        "pip install -q transformers datasets\n",
        "```\n",
        "\n",
        "```bash\n",
        "pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "pip install torchvision\n",
        "```\n",
        "\n",
        "```bash\n",
        "sudo apt install tesseract-ocr\n",
        "pip install -q pytesseract\n",
        "```\n",
        "\n",
        "Once you have installed all of the dependencies, restart your runtime.\n",
        "\n",
        "We encourage you to share your model with the community. Log in to your Hugging Face account to upload it to the ğŸ¤— Hub.\n",
        "When prompted, enter your token to log in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF6VApnwD5cF"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb2A9tXoD5cG"
      },
      "source": [
        "Let's define some global variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jMx5BE7D5cG"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKTi2rx-D5cG"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u3pvT-jD5cH"
      },
      "source": [
        "In this guide we use a small sample of preprocessed DocVQA that you can find on ğŸ¤— Hub. If you'd like to use the full\n",
        "DocVQA dataset, you can register and download it on [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17). If you do so, to\n",
        "proceed with this guide check out [how to load files into a ğŸ¤— dataset](https://huggingface.co/docs/datasets/loading#local-and-remote-files)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLqh5RdmD5cH",
        "outputId": "802d1fa8-d64d-45ac-9948-a8e5c2ddc478"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"nielsr/docvqa_1200_examples\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSH2g4EUD5cI"
      },
      "source": [
        "As you can see, the dataset is split into train and test sets already. Take a look at a random example to familiarize\n",
        "yourself with the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSfvzPKRD5cI"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"].features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8tEzo16D5cI"
      },
      "source": [
        "Here's what the individual fields represent:\n",
        "* `id`: the example's id\n",
        "* `image`: a PIL.Image.Image object containing the document image\n",
        "* `query`: the question string - natural language asked question, in several languages\n",
        "* `answers`: a list of correct answers provided by human annotators\n",
        "* `words` and `bounding_boxes`: the results of OCR, which we will not use here\n",
        "* `answer`: an answer matched by a different model which we will not use here\n",
        "\n",
        "Let's leave only English questions, and drop the `answer` feature which appears to contain predictions by another model.\n",
        "We'll also take the first of the answers from the set provided by the annotators. Alternatively, you can randomly sample it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAC9n7x2D5cI"
      },
      "outputs": [],
      "source": [
        "updated_dataset = dataset.map(lambda example: {\"question\": example[\"query\"][\"en\"]}, remove_columns=[\"query\"])\n",
        "updated_dataset = updated_dataset.map(\n",
        "    lambda example: {\"answer\": example[\"answers\"][0]}, remove_columns=[\"answer\", \"answers\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRm2MPDfD5cJ"
      },
      "source": [
        "Note that the LayoutLMv2 checkpoint that we use in this guide has been trained with `max_position_embeddings = 512` (you can\n",
        "find this information in the [checkpoint's `config.json` file](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)).\n",
        "We can truncate the examples but to avoid the situation where the answer might be at the end of a large document and end up truncated,\n",
        "here we'll remove the few examples where the embedding is likely to end up longer than 512.\n",
        "If most of the documents in your dataset are long, you can implement a sliding window strategy - check out [this notebook](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYErshITD5cJ"
      },
      "outputs": [],
      "source": [
        "updated_dataset = updated_dataset.filter(lambda x: len(x[\"words\"]) + len(x[\"question\"].split()) < 512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n75uuWmKD5cJ"
      },
      "source": [
        "At this point let's also remove the OCR features from this dataset. These are a result of OCR for fine-tuning a different\n",
        "model. They would still require some processing if we wanted to use them, as they do not match the input requirements\n",
        "of the model we use in this guide. Instead, we can use the [LayoutLMv2Processor](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor) on the original data for both OCR and\n",
        "tokenization. This way we'll get the inputs that match model's expected input. If you want to process images manually,\n",
        "check out the [`LayoutLMv2` model documentation](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv2) to learn what input format the model expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jbNCETPD5cJ"
      },
      "outputs": [],
      "source": [
        "updated_dataset = updated_dataset.remove_columns(\"words\")\n",
        "updated_dataset = updated_dataset.remove_columns(\"bounding_boxes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT7For4CD5cJ"
      },
      "source": [
        "Finally, the data exploration won't be complete if we don't peek at an image example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0NJXnTQD5cJ"
      },
      "outputs": [],
      "source": [
        "updated_dataset[\"train\"][11][\"image\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDa171UD5cK"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/docvqa_example.jpg\" alt=\"DocVQA Image Example\"/>\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrxLma4D5cK"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmlURjo8D5cK"
      },
      "source": [
        "The Document Question Answering task is a multimodal task, and you need to make sure that the inputs from each modality\n",
        "are preprocessed according to the model's expectations. Let's start by loading the [LayoutLMv2Processor](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor), which internally combines an image processor that can handle image data and a tokenizer that can encode text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgVTRW8vD5cK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POUulx6fD5cK"
      },
      "source": [
        "### Preprocessing document images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IvyX3u6D5cK"
      },
      "source": [
        "First, let's prepare the document images for the model with the help of the `image_processor` from the processor.\n",
        "By default, image processor resizes the images to 224x224, makes sure they have the correct order of color channels,\n",
        "applies OCR with tesseract to get words and normalized bounding boxes. In this tutorial, all of these defaults are exactly what we need.\n",
        "Write a function that applies the default image processing to a batch of images and returns the results of OCR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2s3ZlvCD5cK"
      },
      "outputs": [],
      "source": [
        "image_processor = processor.image_processor\n",
        "\n",
        "\n",
        "def get_ocr_words_and_boxes(examples):\n",
        "    images = [image.convert(\"RGB\") for image in examples[\"image\"]]\n",
        "    encoded_inputs = image_processor(images)\n",
        "\n",
        "    examples[\"image\"] = encoded_inputs.pixel_values\n",
        "    examples[\"words\"] = encoded_inputs.words\n",
        "    examples[\"boxes\"] = encoded_inputs.boxes\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4SWTJxvD5cK"
      },
      "source": [
        "To apply this preprocessing to the entire dataset in a fast way, use [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpyrt3UFD5cK"
      },
      "outputs": [],
      "source": [
        "dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg-SGyVuD5cL"
      },
      "source": [
        "### Preprocessing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGgbeGoqD5cL"
      },
      "source": [
        "Once we have applied OCR to the images, we need to encode the text part of the dataset to prepare it for the model.\n",
        "This involves converting the words and boxes that we got in the previous step to token-level `input_ids`, `attention_mask`,\n",
        "`token_type_ids` and `bbox`. For preprocessing text, we'll need the `tokenizer` from the processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek1bTsRKD5cL"
      },
      "outputs": [],
      "source": [
        "tokenizer = processor.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAf9sTYVD5cL"
      },
      "source": [
        "On top of the preprocessing mentioned above, we also need to add the labels for the model. For `xxxForQuestionAnswering` models\n",
        "in ğŸ¤— Transformers, the labels consist of the `start_positions` and `end_positions`, indicating which token is at the\n",
        "start and which token is at the end of the answer.\n",
        "\n",
        "Let's start with that. Define a helper function that can find a sublist (the answer split into words) in a larger list (the words list).\n",
        "\n",
        "This function will take two lists as input, `words_list` and `answer_list`. It will then iterate over the `words_list` and check\n",
        "if the current word in the `words_list` (words_list[i]) is equal to the first word of answer_list (answer_list[0]) and if\n",
        "the sublist of `words_list` starting from the current word and of the same length as `answer_list` is equal `to answer_list`.\n",
        "If this condition is true, it means that a match has been found, and the function will record the match, its starting index (idx),\n",
        "and its ending index (idx + len(answer_list) - 1). If more than one match was found, the function will return only the first one.\n",
        "If no match is found, the function returns (`None`, 0, and 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7s25KEKD5cL"
      },
      "outputs": [],
      "source": [
        "def subfinder(words_list, answer_list):\n",
        "    matches = []\n",
        "    start_indices = []\n",
        "    end_indices = []\n",
        "    for idx, i in enumerate(range(len(words_list))):\n",
        "        if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:\n",
        "            matches.append(answer_list)\n",
        "            start_indices.append(idx)\n",
        "            end_indices.append(idx + len(answer_list) - 1)\n",
        "    if matches:\n",
        "        return matches[0], start_indices[0], end_indices[0]\n",
        "    else:\n",
        "        return None, 0, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbpEhfQcD5cL"
      },
      "source": [
        "To illustrate how this function finds the position of the answer, let's use it on an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUcB66PmD5cM",
        "outputId": "f317422e-f4e5-46ca-a7b5-a6360f9c313d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Question:  Who is in  cc in this letter?\n",
              "Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'ce:', 't.f.', 'riehl', 'from:', '.', 'c.j.', 'cook', 'date:', 'may', '8,', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'graup', 'is', 'to', 'develop', 'marketable', 'nove!', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'incorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorming', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'canventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'cne.', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', '*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', 'future', 'smoking.', 'Â«short', 'cigarette,', 'tobacco', 'section', '30', 'mm.', 'Â«extremely', 'fast', 'buming', 'cigarette.', 'Â«novel', 'cigarette', 'constructions', 'that', 'permit', 'a', 'significant', 'reduction', 'iretobacco', 'weight', 'while', 'maintaining', 'smoking', 'mechanics', 'and', 'visual', 'characteristics.', 'higher', 'basis', 'weight', 'paper:', 'potential', 'reduction', 'in', 'tobacco', 'weight.', 'Â«more', 'rigid', 'tobacco', 'column;', 'stiffing', 'agent', 'for', 'tobacco;', 'e.g.', 'starch', '*colored', 'tow', 'and', 'cigarette', 'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'or', 'in', 'an', 'ebony', 'and', 'ivory', 'brand', 'containing', 'a', 'mixture', 'of', 'all', 'black', '(black', 'paper', 'and', 'tow)', 'and', 'ail', 'white', 'cigarettes.', '499150498']\n",
              "Answer:  T.F. Riehl\n",
              "start_index 17\n",
              "end_index 18"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = dataset_with_ocr[\"train\"][1]\n",
        "words = [word.lower() for word in example[\"words\"]]\n",
        "match, word_idx_start, word_idx_end = subfinder(words, example[\"answer\"].lower().split())\n",
        "print(\"Question: \", example[\"question\"])\n",
        "print(\"Words:\", words)\n",
        "print(\"Answer: \", example[\"answer\"])\n",
        "print(\"start_index\", word_idx_start)\n",
        "print(\"end_index\", word_idx_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxUOaIYED5cM"
      },
      "source": [
        "Once examples are encoded, however, they will look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wShfGL4D5cM",
        "outputId": "88117f34-841f-4399-9dc0-9a3fb73aad8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[CLS] who is in cc in this letter? [SEP] wie baw brown & williamson tobacco corporation research & development ..."
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer(example[\"question\"], example[\"words\"], example[\"boxes\"])\n",
        "tokenizer.decode(encoding[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k9-q1-fD5cM"
      },
      "source": [
        "We'll need to find the position of the answer in the encoded input.\n",
        "* `token_type_ids` tells us which tokens are part of the question, and which ones are part of the document's words.\n",
        "* `tokenizer.cls_token_id` will help find the special token at the beginning of the input.\n",
        "* `word_ids` will help match the answer found in the original `words` to the same answer in the full encoded input and determine\n",
        "the start/end position of the answer in the encoded input.\n",
        "\n",
        "With that in mind, let's create a function to encode a batch of examples in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzdWIPL-D5cM"
      },
      "outputs": [],
      "source": [
        "def encode_dataset(examples, max_length=512):\n",
        "    questions = examples[\"question\"]\n",
        "    words = examples[\"words\"]\n",
        "    boxes = examples[\"boxes\"]\n",
        "    answers = examples[\"answer\"]\n",
        "\n",
        "    # encode the batch of examples and initialize the start_positions and end_positions\n",
        "    encoding = tokenizer(questions, words, boxes, max_length=max_length, padding=\"max_length\", truncation=True)\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    # loop through the examples in the batch\n",
        "    for i in range(len(questions)):\n",
        "        cls_index = encoding[\"input_ids\"][i].index(tokenizer.cls_token_id)\n",
        "\n",
        "        # find the position of the answer in example's words\n",
        "        words_example = [word.lower() for word in words[i]]\n",
        "        answer = answers[i]\n",
        "        match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())\n",
        "\n",
        "        if match:\n",
        "            # if match is found, use `token_type_ids` to find where words start in the encoding\n",
        "            token_type_ids = encoding[\"token_type_ids\"][i]\n",
        "            token_start_index = 0\n",
        "            while token_type_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(encoding[\"input_ids\"][i]) - 1\n",
        "            while token_type_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]\n",
        "            start_position = cls_index\n",
        "            end_position = cls_index\n",
        "\n",
        "            # loop over word_ids and increase `token_start_index` until it matches the answer position in words\n",
        "            # once it matches, save the `token_start_index` as the `start_position` of the answer in the encoding\n",
        "            for id in word_ids:\n",
        "                if id == word_idx_start:\n",
        "                    start_position = token_start_index\n",
        "                else:\n",
        "                    token_start_index += 1\n",
        "\n",
        "            # similarly loop over `word_ids` starting from the end to find the `end_position` of the answer\n",
        "            for id in word_ids[::-1]:\n",
        "                if id == word_idx_end:\n",
        "                    end_position = token_end_index\n",
        "                else:\n",
        "                    token_end_index -= 1\n",
        "\n",
        "            start_positions.append(start_position)\n",
        "            end_positions.append(end_position)\n",
        "\n",
        "        else:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "\n",
        "    encoding[\"image\"] = examples[\"image\"]\n",
        "    encoding[\"start_positions\"] = start_positions\n",
        "    encoding[\"end_positions\"] = end_positions\n",
        "\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI3tXdzID5cS"
      },
      "source": [
        "Now that we have this preprocessing function, we can encode the entire dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku6zvk-2D5cT"
      },
      "outputs": [],
      "source": [
        "encoded_train_dataset = dataset_with_ocr[\"train\"].map(\n",
        "    encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr[\"train\"].column_names\n",
        ")\n",
        "encoded_test_dataset = dataset_with_ocr[\"test\"].map(\n",
        "    encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr[\"test\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6kRP4JMD5cT"
      },
      "source": [
        "Let's check what the features of the encoded dataset look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRPNFFDSD5cT",
        "outputId": "1a5c4f07-4c63-4c52-f261-7334de06a9b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n",
              " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
              " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
              " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
              " 'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
              " 'start_positions': Value(dtype='int64', id=None),\n",
              " 'end_positions': Value(dtype='int64', id=None)}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_train_dataset.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q12sMIl8D5cT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uTBfPxcD5cT"
      },
      "source": [
        "Evaluation for document question answering requires a significant amount of postprocessing. To avoid taking up too much\n",
        "of your time, this guide skips the evaluation step. The [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) still calculates the evaluation loss during training so\n",
        "you're not completely in the dark about your model's performance. Extractive question answering is typically evaluated using F1/exact match.\n",
        "If you'd like to implement it yourself, check out the [Question Answering chapter](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing)\n",
        "of the Hugging Face course for inspiration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1I5IFBxD5cU"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxuJKduQD5cU"
      },
      "source": [
        "Congratulations! You've successfully navigated the toughest part of this guide and now you are ready to train your own model.\n",
        "Training involves the following steps:\n",
        "* Load the model with [AutoModelForDocumentQuestionAnswering](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForDocumentQuestionAnswering) using the same checkpoint as in the preprocessing.\n",
        "* Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "* Define a function to batch examples together, here the [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) will do just fine\n",
        "* Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, and data collator.\n",
        "* Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjoAwom9D5cU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForDocumentQuestionAnswering\n",
        "\n",
        "model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpzFFOTcD5cU"
      },
      "source": [
        "In the [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) use `output_dir` to specify where to save your model, and configure hyperparameters as you see fit.\n",
        "If you wish to share your model with the community, set `push_to_hub` to `True` (you must be signed in to Hugging Face to upload your model).\n",
        "In this case the `output_dir` will also be the name of the repo where your model checkpoint will be pushed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbF_6GPED5cU"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# REPLACE THIS WITH YOUR REPO ID\n",
        "repo_id = \"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=repo_id,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=20,\n",
        "    save_steps=200,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    learning_rate=5e-5,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlKbadS4D5cU"
      },
      "source": [
        "Define a simple data collator to batch examples together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPKymyCHD5cU"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mpDPSmyD5cU"
      },
      "source": [
        "Finally, bring everything together, and call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqkkkY4YD5cV"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=encoded_train_dataset,\n",
        "    eval_dataset=encoded_test_dataset,\n",
        "    processing_class=processor,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kkJevL9D5cV"
      },
      "source": [
        "To add the final model to ğŸ¤— Hub, create a model card and call `push_to_hub`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8bl39YND5cV"
      },
      "outputs": [],
      "source": [
        "trainer.create_model_card()\n",
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxJrDYvXD5cV"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CxcAzZgD5cV"
      },
      "source": [
        "Now that you have finetuned a LayoutLMv2 model, and uploaded it to the ğŸ¤— Hub, you can use it for inference. The simplest\n",
        "way to try out your finetuned model for inference is to use it in a [Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline).\n",
        "\n",
        "Let's take an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnjjtJeOD5cV",
        "outputId": "baca7df4-f921-4e9c-b7d4-0f55d3d4a7ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Who is â€˜presidingâ€™ TRRF GENERAL SESSION (PART 1)?'\n",
              "['TRRF Vice President', 'lee a. waller']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = dataset[\"test\"][2]\n",
        "question = example[\"query\"][\"en\"]\n",
        "image = example[\"image\"]\n",
        "print(question)\n",
        "print(example[\"answers\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYh_EE5WD5cV"
      },
      "source": [
        "Next, instantiate a pipeline for\n",
        "document question answering with your model, and pass the image + question combination to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTT_DHR5D5cW",
        "outputId": "e40f83b3-76e3-4511-a0d0-00804cf10ac0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'score': 0.9949808120727539,\n",
              "  'answer': 'Lee A. Waller',\n",
              "  'start': 55,\n",
              "  'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\"document-question-answering\", model=\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n",
        "qa_pipeline(image, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_eEZ7onD5cW"
      },
      "source": [
        "You can also manually replicate the results of the pipeline if you'd like:\n",
        "1. Take an image and a question, prepare them for the model using the processor from your model.\n",
        "2. Forward the result or preprocessing through the model.\n",
        "3. The model returns `start_logits` and `end_logits`, which indicate which token is at the start of the answer and\n",
        "which token is at the end of the answer. Both have shape (batch_size, sequence_length).\n",
        "4. Take an argmax on the last dimension of both the `start_logits` and `end_logits` to get the predicted `start_idx` and `end_idx`.\n",
        "5. Decode the answer with the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgIbYK2-D5cW",
        "outputId": "a709f35f-58a4-4e58-f223-6c9019fc7d38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'lee a. waller'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor\n",
        "from transformers import AutoModelForDocumentQuestionAnswering\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n",
        "model = AutoModelForDocumentQuestionAnswering.from_pretrained(\"MariaK/layoutlmv2-base-uncased_finetuned_docvqa\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    encoding = processor(image.convert(\"RGB\"), question, return_tensors=\"pt\")\n",
        "    outputs = model(**encoding)\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "    predicted_start_idx = start_logits.argmax(-1).item()\n",
        "    predicted_end_idx = end_logits.argmax(-1).item()\n",
        "\n",
        "processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}